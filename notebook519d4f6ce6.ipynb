{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# ignore warning \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.applications import MobileNetV2\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "# from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = glob.glob(\"../input/dataset-characters/dataset_characters/**/*.jpg\")\n",
    "\n",
    "cols=4\n",
    "rows=3\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.rcParams.update({\"font.size\":14})\n",
    "grid = gridspec.GridSpec(ncols=cols,nrows=rows,figure=fig)\n",
    "\n",
    "# create a random list of images will be displayed\n",
    "np.random.seed(45)\n",
    "print(len(dataset_paths))\n",
    "rand = np.random.randint(0,len(dataset_paths),size=(cols*rows))\n",
    "print(rand)\n",
    "# Plot image\n",
    "for i in range(cols*rows):\n",
    "    fig.add_subplot(grid[i])\n",
    "    image = load_img(dataset_paths[rand[i]])\n",
    "    label = dataset_paths[rand[i]].split(os.path.sep)[-2]\n",
    "    plt.title('\"{:s}\"'.format(label))\n",
    "    plt.axis(False)\n",
    "    plt.imshow(image)\n",
    "\n",
    "plt.savefig(\"Visualize_dataset.jpg\",dpi=300)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arange input data and corresponding labels\n",
    "X=[]\n",
    "labels=[]\n",
    "\n",
    "for image_path in dataset_paths:\n",
    "  label = image_path.split(os.path.sep)[-2]\n",
    "  image=load_img(image_path,target_size=(80,80))\n",
    "  image=img_to_array(image)\n",
    "\n",
    "  X.append(image)\n",
    "  labels.append(label)\n",
    "\n",
    "X = np.array(X,dtype=\"float16\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"[INFO] Find {:d} images with {:d} classes\".format(len(X),len(set(labels))))\n",
    "\n",
    "\n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelEncoder()\n",
    "lb.fit(labels)\n",
    "labels = lb.transform(labels)\n",
    "y = to_categorical(labels)\n",
    "\n",
    "# save label file so we can use in another script\n",
    "np.save('license_character_classes.npy', lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 10% of data as validation set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augumentation\n",
    "image_gen = ImageDataGenerator(rotation_range=10,\n",
    "                              width_shift_range=0.1,\n",
    "                              height_shift_range=0.1,\n",
    "                              shear_range=0.1,\n",
    "                              zoom_range=0.1,\n",
    "                              fill_mode=\"nearest\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=1e-4,decay=1e-4/25, training=False,output_shape=y.shape[1]):\n",
    "    baseModel = ResNet50(weights=\"imagenet\", \n",
    "                            include_top=False,\n",
    "                            input_tensor=Input(shape=(80, 80, 3)))\n",
    "\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size=(3, 3))(headModel)\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "    headModel = Dropout(0.5)(headModel)\n",
    "    headModel = Dense(output_shape, activation=\"softmax\")(headModel)\n",
    "    \n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "    \n",
    "    if training:\n",
    "        # define trainable lalyer\n",
    "        for layer in baseModel.layers:\n",
    "            layer.trainable = True\n",
    "        # compile model\n",
    "        optimizer = Adam(lr=lr, decay = decay)\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['acc',f1_m,precision_m, recall_m])    \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilaize initial hyperparameter\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "model = create_model(lr=INIT_LR, decay=INIT_LR/EPOCHS,training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "my_checkpointer = [\n",
    "                EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "                ModelCheckpoint(filepath=\"License_character_recognition.h5\", verbose=1, save_weights_only=True)\n",
    "                ]\n",
    "\n",
    "result = model.fit(image_gen.flow(trainX, trainY, batch_size=BATCH_SIZE), \n",
    "                   steps_per_epoch=len(trainX) // BATCH_SIZE, \n",
    "                   validation_data=(testX, testY), \n",
    "                   validation_steps=len(testX) // BATCH_SIZE, \n",
    "                   epochs=EPOCHS, callbacks=my_checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,5))\n",
    "grid=gridspec.GridSpec(ncols=2,nrows=1,figure=fig)\n",
    "fig.add_subplot(grid[0])\n",
    "plt.plot(result.history['acc'], label='training accuracy')\n",
    "plt.plot(result.history['val_acc'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(grid[1])\n",
    "plt.plot(result.history['loss'], label='training loss')\n",
    "plt.plot(result.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "#plt.savefig(\"Training_result.jpg\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,5))\n",
    "grid=gridspec.GridSpec(ncols=2,nrows=1,figure=fig)\n",
    "fig.add_subplot(grid[0])\n",
    "plt.plot(result.history['f1_m'], label='training f1score')\n",
    "plt.plot(result.history['val_f1_m'], label='val f1score')\n",
    "plt.title('F1 score')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('f1 score')\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(grid[1])\n",
    "plt.plot(result.history['precision_m'], label='train precision')\n",
    "plt.plot(result.history['val_precision_m'], label='val precision')\n",
    "plt.title('Precision')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('precision')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,5))\n",
    "grid=gridspec.GridSpec(ncols=2,nrows=1,figure=fig)\n",
    "fig.add_subplot(grid[0])\n",
    "plt.plot(result.history['recall_m'], label='train recall')\n",
    "plt.plot(result.history['val_recall_m'], label='val recall')\n",
    "plt.title('Recall')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('recall')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model architectur as json file\n",
    "model_json = model.to_json()\n",
    "with open(\"Resnet_character_recognition.json\", \"w\") as json_file:\n",
    "  json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "plate_cascade = cv2.CascadeClassifier('../input/ai-indian-license-plate-recognition-data/indian_license_plate.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_plate(img, text=''): # the function detects and perfors blurring on the number plate.\n",
    "    plate_img = img.copy()\n",
    "    roi = img.copy()\n",
    "    plate_rect = plate_cascade.detectMultiScale(plate_img, scaleFactor = 1.2, minNeighbors = 7) # detects numberplates and returns the coordinates and dimensions of detected license plate's contours.\n",
    "    for (x,y,w,h) in plate_rect:\n",
    "        roi_ = roi[y:y+h, x:x+w, :] # extracting the Region of Interest of license plate for blurring.\n",
    "        plate = roi[y:y+h, x:x+w, :]\n",
    "        cv2.rectangle(plate_img, (x+2,y), (x+w-3, y+h-5), (51,181,155), 3) # finally representing the detected contours by drawing rectangles around the edges.\n",
    "    if text!='':\n",
    "        plate_img = cv2.putText(plate_img, text, (x-w//2,y-h//2), \n",
    "                                cv2.FONT_HERSHEY_COMPLEX_SMALL , 0.5, (51,181,155), 1, cv2.LINE_AA)\n",
    "        \n",
    "    return plate_img, plate # returning the processed image.\n",
    "# Testing the above function\n",
    "def display(img_, title=''):\n",
    "    img = cv2.cvtColor(img_, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../input/ai-indian-license-plate-recognition-data/car.jpg')\n",
    "# Getting plate prom the processed image\n",
    "output_img, plate = detect_plate(img)\n",
    "display(output_img, 'detected license plate in the input image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plate, 'extracted license plate from the image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match contours to license plate or character template\n",
    "def find_contours(dimensions, img) :\n",
    "\n",
    "    # Find all contours in the image\n",
    "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve potential dimensions\n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "    \n",
    "    # Check largest 5 or  15 contours for license plate or character respectively\n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "    \n",
    "    ii = cv2.imread('contour.jpg')\n",
    "    \n",
    "    x_cntr_list = []\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    for cntr in cntrs :\n",
    "        # detects contour in binary image and returns the coordinates of rectangle enclosing it\n",
    "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
    "        \n",
    "        # checking the dimensions of the contour to filter out the characters by contour's size\n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
    "\n",
    "            char_copy = np.zeros((44,24))\n",
    "            # extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "            \n",
    "            cv2.rectangle(ii, (intX,intY), (intWidth+intX, intY+intHeight), (50,21,200), 2)\n",
    "            plt.imshow(ii, cmap='gray')\n",
    "\n",
    "            # Make result formatted for classification: invert colors\n",
    "            char = cv2.subtract(255, char)\n",
    "\n",
    "            # Resize the image to 24x44 with black border\n",
    "            char_copy[2:42, 2:22] = char\n",
    "            char_copy[0:2, :] = 0\n",
    "            char_copy[:, 0:2] = 0\n",
    "            char_copy[42:44, :] = 0\n",
    "            char_copy[:, 22:24] = 0\n",
    "\n",
    "            img_res.append(char_copy) # List that stores the character's binary image (unsorted)\n",
    "            \n",
    "    # Return characters on ascending order with respect to the x-coordinate (most-left character first)\n",
    "            \n",
    "    plt.show()\n",
    "    # arbitrary function that stores sorted list of character indeces\n",
    "    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])\n",
    "    img_res_copy = []\n",
    "    for idx in indices:\n",
    "        img_res_copy.append(img_res[idx])# stores character images according to their index\n",
    "    img_res = np.array(img_res_copy)\n",
    "\n",
    "    return img_res\n",
    "# Find characters in the resulting images\n",
    "def segment_characters(image) :\n",
    "\n",
    "    # Preprocess cropped license plate image\n",
    "    img_lp = cv2.resize(image, (333, 75))\n",
    "    img_gray_lp = cv2.cvtColor(img_lp, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary_lp = cv2.threshold(img_gray_lp, 200, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    img_binary_lp = cv2.erode(img_binary_lp, (3,3))\n",
    "    img_binary_lp = cv2.dilate(img_binary_lp, (3,3))\n",
    "\n",
    "    LP_WIDTH = img_binary_lp.shape[0]\n",
    "    LP_HEIGHT = img_binary_lp.shape[1]\n",
    "\n",
    "    # Make borders white\n",
    "    img_binary_lp[0:3,:] = 255\n",
    "    img_binary_lp[:,0:3] = 255\n",
    "    img_binary_lp[72:75,:] = 255\n",
    "    img_binary_lp[:,330:333] = 255\n",
    "\n",
    "    # Estimations of character contours sizes of cropped license plates\n",
    "    dimensions = [LP_WIDTH/6,\n",
    "                       LP_WIDTH/2,\n",
    "                       LP_HEIGHT/10,\n",
    "                       2*LP_HEIGHT/3]\n",
    "    plt.imshow(img_binary_lp, cmap='gray')\n",
    "    plt.show()\n",
    "    cv2.imwrite('contour.jpg',img_binary_lp)\n",
    "\n",
    "    # Get contours within cropped license plate\n",
    "    char_list = find_contours(dimensions, img_binary_lp)\n",
    "\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the segmented characters\n",
    "char = segment_characters(plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(char[i], cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "# from local_utils import detect_lp\n",
    "from os.path import splitext,basename\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('./Resnet_character_recognition.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights(\"./License_character_recognition.h5\")\n",
    "print(\"[INFO] Model loaded successfully...\")\n",
    "\n",
    "labels = LabelEncoder()\n",
    "labels.classes_ = np.load('./license_character_classes.npy')\n",
    "print(\"[INFO] Labels loaded successfully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing input images and pedict with model\n",
    "def predict_from_model(image,model,labels):\n",
    "    image = cv2.resize(image,(80,80))\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "    prediction = labels.inverse_transform([np.argmax(model.predict(image[np.newaxis,:]))])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,3))\n",
    "cols = len(char)\n",
    "grid = gridspec.GridSpec(ncols=cols,nrows=1,figure=fig)\n",
    "\n",
    "final_string = ''\n",
    "for i,character in enumerate(char):\n",
    "    fig.add_subplot(grid[i])\n",
    "    title = np.array2string(predict_from_model(character,model,labels))\n",
    "    plt.title('{}'.format(title.strip(\"'[]\"),fontsize=20))\n",
    "    final_string+=title.strip(\"'[]\")\n",
    "    plt.axis(False)\n",
    "    plt.imshow(character,cmap='gray')\n",
    "\n",
    "print(final_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
